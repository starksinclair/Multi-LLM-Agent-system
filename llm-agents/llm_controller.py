import json
from enum import Enum
from typing import Optional, Dict

from pydantic import BaseModel
from llm_provider import BaseLLM, LLMResponse
from mcp_services.mcp_web_search_server import  MCPWebSearchServer
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMTask(BaseModel):
    task_id: str
    description: str
    prompt: str
    system_prompt: Optional[str] = None
    requires_search: bool = False
    requires_search_query_refinement: bool = False
    requires_search_result_refinement: bool = False

class LLMRole(Enum):
    QUERY_REFINER = "query_refiner"
    RESEARCHER = "researcher"
    ANALYZER = "analyzer"
    SYNTHESIZER = "synthesizer"
    VALIDATOR = "validator"


def _get_system_prompts():
    return {
        LLMRole.QUERY_REFINER: """You are an expert medical search query optimizer. Your goal is to transform user questions into precise and effective search queries for medical research.
        Focus on using accurate medical terminology, adding relevant keywords, and formulating it for direct search result relevance (e.g., symptoms, treatments, drug info, disease mechanisms).
        The output should be only the refined query string, with no additional text or explanation. Do not include any conversational phrases or disclaimers.
        """,
        LLMRole.RESEARCHER: """You are a medical research agent. Your role is to:
        1. Analyze medical questions and identify key search terms
        2. Review search results and extract relevant medical information
        3. Focus on evidence-based information from reputable sources
        4. Always mention that medical information should be verified with healthcare professionals
        5. Be precise and factual in your analysis""",

        LLMRole.ANALYZER: """You are a medical analysis agent. Your role is to:
        1. Analyze medical information for accuracy and completeness
        2. Identify potential gaps or inconsistencies in information
        3. Cross-reference information from multiple sources
        4. Assess the reliability of sources and information quality
        5. Highlight any conflicting information found""",

        LLMRole.SYNTHESIZER: """You are a medical synthesis agent. Your role is to:
        1. Combine information from multiple sources into coherent responses
        2. Structure medical information clearly and logically
        3. Ensure balanced presentation of different viewpoints
        4. Create comprehensive yet accessible explanations
        5. Always include appropriate medical disclaimers""",

        LLMRole.VALIDATOR: """You are a medical validation agent. Your role is to:
        1. Review final medical responses for accuracy and safety
        2. Ensure appropriate disclaimers are included
        3. Check that responses don't provide specific medical advice
        4. Validate that information is presented responsibly
        5. Flag any potentially harmful or misleading content"""
    }


class MedicalLLMController:
    def __init__(self, role: LLMRole, llm: BaseLLM, mcp_server:  MCPWebSearchServer ):
        self.role = role
        self.llm = llm
        self.mcp_server = mcp_server
        self.system_prompts = _get_system_prompts()

    async def execute_task(self, task: LLMTask, search_context: Optional[Dict] = None) -> LLMResponse:
        """
        Execute a given task using the specified LLM and MCP server.
        Args:
            task (LLMTask): The task to execute.
            llm (BaseLLM): The LLM to use for generating responses.
            mcp_server (MCPServer): The MCP server for handling web searches.

        Returns:
            LLMResponse: The response generated by the LLM.
            :param task:
            :param search_context:
        """
        full_prompt = task.prompt
        if task.requires_search and search_context:
            search_info = f"\n\nSearch Results Context:\n{json.dumps(search_context, indent=2)}"
            full_prompt += search_info

            # Get system prompt for this role
        system_prompt = task.system_prompt or self.system_prompts.get(self.role, "")

        # Generate response using the LLM
        response = await self.llm.generate_response(full_prompt, system_prompt)

        logger.info(f"Agent {self.role.value} ({self.llm.get_provider().value}) completed task: {task.task_id}")

        return response